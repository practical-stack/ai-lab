---
title: "결과 분석"
description: "표준 및 엣지 케이스 프롬프트에 걸친 Hook 구성의 데이터 기반 비교와 오탐 및 분산 분석"
type: tutorial
tags: [AI, Testing, BestPractice]
order: 4
depends_on: [./03-eval-harness.ko.md]
related: [./04-results-analysis.en.md, ./05-implementation-guide.ko.md]
---

# Module 4: 결과 분석

> 약 250회 호출 데이터가 명확한 승자와 놀라운 패턴을 보여줍니다

## 학습 목표

이 모듈을 완료하면 다음을 할 수 있습니다:
- 5가지 Hook 구성 전체의 활성화율 비교
- 표준 프롬프트와 엣지 케이스 프롬프트 결과의 차이 분석
- 오탐율과 그 실질적 영향 평가
- 활성화 성공을 이끄는 키워드 패턴 식별
- LLM 평가 결과의 분산과 통계적 신뢰도 평가

---

## 4.1 표준 프롬프트 결과

표준 프롬프트는 Skill 설명과 겹치는 키워드를 포함합니다 — 활성화의 "정상 경로"입니다.

### 종합 결과

| 구성 | 실행 1 | 실행 2 | 실행 3 | 평균 | 분산 |
|------|--------|--------|--------|------|------|
| none | 50% | 55% | 52% | 52.3% | 낮음 |
| simple | 50% | 59% | 53% | 54.0% | 중간 |
| **forced-eval** | **100%** | **100%** | **100%** | **100%** | **없음** |
| llm-eval | 100% | 100% | 100% | 100% | 없음 |
| type-prompt | 41% | 55% | 50% | 48.7% | 높음 |

### 주요 관찰

**1. 명확한 계층 분리**

```
Tier 1 (100%):  forced-eval, llm-eval
Tier 2 (~50%):  none, simple, type-prompt
```

중간 지대가 없습니다. Hook은 문제를 완전히 해결하거나 전혀 도움이 되지 않습니다.

**2. Simple echo는 개선을 제공하지 않음**

Claude의 컨텍스트에 리마인더를 주입했음에도 simple echo Hook은 기준선 분산 범위 내에서 작동합니다(50-59% vs 50-55%). 지시가 동작을 변경하기에 너무 약합니다.

**3. Type-prompt는 기준선보다 나쁨**

`type: prompt` Hook은 때때로 기준선 *이하*로 수행됩니다(한 실행에서 41%). 이는 프롬프트 주입이 Claude의 기본 Skill 확인 동작을 향상시키기보다 방해할 수 있음을 시사합니다.

---

## 4.2 엣지 케이스 프롬프트 결과

엣지 케이스 프롬프트는 더 어렵게 설계되었습니다 — Skill과 의미적으로 관련된 작업을 설명하지만 키워드 겹침이 없습니다.

### 엣지 케이스 분류

| 분류 | 예시 프롬프트 | 예상 Skill | 난이도 |
|------|-------------|-----------|--------|
| **의미만** | "add form actions" | svelte-forms | `$state` 키워드 없음 |
| **비매칭** | "update the README" | 없음 | 어떤 것도 활성화하면 안 됨 |
| **모호** | "improve the page" | 불확실 | 여러 Skill이 적용 가능 |
| **다른 도메인** | "optimize database queries" | 없음 | 완전히 무관 |

### 엣지 케이스 결과

| 구성 | 활성화 (매칭) | 오탐 (비매칭) |
|------|-------------|-------------|
| none | N/A (미테스트) | N/A |
| simple | N/A | N/A |
| **forced-eval** | **75%** | **0% (5개 중 0개)** |
| llm-eval | 67% | **80% (5개 중 4개)** |
| type-prompt | N/A | N/A |

### Forced-Eval 엣지 케이스 분석

매칭 엣지 케이스에서 75% 활성화(표준의 100%에서 하락)는 forced-eval조차 완전히 부재한 키워드 신호를 극복할 수 없음을 보여줍니다. 하지만 결정적으로:

- **오활성화 없음**: Skill이 매칭되지 않을 때 Claude는 모든 Skill에 대해 올바르게 NO라고 답함
- **의도적 추론**: YES/NO 평가 단계가 관련성에 대한 명확한 추론을 생성
- **우아한 성능 저하**: 어려운 프롬프트에서 75%는 쉬운 프롬프트의 기준선 50%보다 훨씬 우수

### LLM-Eval 엣지 케이스 분석

비매칭 프롬프트에서 80% 오탐율이 핵심 결함입니다:

```
프롬프트: "update the README with project description"
예상: Skill 활성화 없음 (README는 Svelte Skill과 무관)
Haiku 사전 분류: "ACTIVATE: svelte-routing, svelte-forms"  ← 오류
Claude: 불필요하게 두 Skill을 모두 활성화
```

Haiku의 사전 분류는 너무 공격적입니다 — 어떤 프롬프트와 사용 가능한 Skill 사이에서 피상적인 연결을 찾습니다. 이는 토큰을 낭비하고 관련 없는 Skill 내용으로 인한 혼란을 유발할 수 있습니다.

---

## 4.3 오탐 분석

오탐은 Claude가 프롬프트와 관련 없는 Skill을 활성화하는 경우입니다.

### 오탐이 중요한 이유

| 영향 | 설명 |
|------|------|
| **토큰 낭비** | 관련 없는 Skill 내용 로드가 컨텍스트 윈도우 소비 |
| **혼란 위험** | 관련 없는 패턴이 구현에 영향을 줄 수 있음 |
| **신뢰 저하** | Claude가 잘못된 Skill을 로드하면 사용자 신뢰 상실 |
| **비용 증가** | 더 많은 토큰 처리 = 더 높은 API 비용 |

### 구성별 오탐율

| 구성 | 표준 | 엣지 (매칭) | 엣지 (비매칭) |
|------|------|-----------|-------------|
| none | 0% | N/A | N/A |
| simple | 0% | N/A | N/A |
| **forced-eval** | **0%** | **0%** | **0%** |
| llm-eval | 0% | 0% | **80%** |
| type-prompt | 0% | N/A | N/A |

### 커밋먼트 메커니즘의 자제 효과

Forced-eval의 오탐율 0%는 YES/NO 열거 단계에서 비롯됩니다:

```
Step 1 - EVALUATE:
  [svelte-routing] - NO - The prompt asks about README documentation,
    not routing
  [svelte-forms] - NO - README updates don't involve form handling
  [svelte-state] - NO - No state management in documentation tasks

Step 2 - ACTIVATE: No skills marked YES.
Step 3 - IMPLEMENT: Proceeding without skill activation.
```

명시적 평가가 Claude에게 각 결정을 정당화하도록 강제합니다. 관련 없는 Skill에 "NO - [이유]"를 작성하는 것은 Claude가 올바르게 처리하는 자연스럽고 저노력 응답입니다. Hook 없는 암묵적 동작과 비교하면, Claude는 그냥... Skill에 대해 전혀 생각하지 않습니다.

---

## 4.4 키워드 패턴 분석

어떤 프롬프트 단어가 활성화를 트리거하는지 분석하면 기저의 매칭 메커니즘이 드러납니다.

### 높은 활성화 키워드

| 키워드/패턴 | 활성화율 | 설명 |
|-----------|---------|------|
| `$state` | 95%+ | Skill 설명과 직접 매칭 |
| `SvelteKit` | 90%+ | 프레임워크 이름 = 강한 신호 |
| `runes` | 85%+ | 특정 기술 용어 |
| `routing` + `svelte` | 90%+ | 복합 매칭 |
| `form` + `validation` | 80%+ | 다중 단어 겹침 |

### 낮은 활성화 키워드

| 키워드/패턴 | 활성화율 | 설명 |
|-----------|---------|------|
| "form actions" | ~30% | 설명에 없는 Svelte 전용 용어 |
| "make reactive" | ~25% | 일반적 개념, 키워드 매칭 없음 |
| "handle input" | ~20% | 매칭하기에 너무 모호 |
| "improve performance" | ~10% | Skill 키워드 겹침 없음 |

### 키워드 임계값

```
                  ┌──────────── 100% forced-eval 라인 ─────────────┐
                  │                                                │
활성화율   100% ─┤████████████████████████████████████████████████│
            80% ─┤                                                │
            60% ─┤     ┌─── 기준선 임계값 ────────┐              │
            50% ─┤─────│─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─│─ ─ ─ ─ ─ ─│
            40% ─┤     └──────────────────────────┘              │
            20% ─┤                                                │
             0% ─┤────────────────────────────────────────────────│
                  └────────────────────────────────────────────────┘
                  강한 키워드 ◀──────────────▶ 의미만
```

Hook 없이 활성화는 키워드 겹침에 따른 기울기를 따릅니다. Forced-eval을 사용하면 키워드 밀도에 관계없이 라인이 100%(또는 그 근처)로 평탄화됩니다.

---

## 4.5 분산 분석

LLM 출력은 비결정적입니다. 분산을 이해하는 것은 평가 결과를 신뢰하기 위해 중요합니다.

### 실행 간 분산

| 구성 | 최소 | 최대 | 범위 | 표준편차 |
|------|------|------|------|---------|
| none | 50% | 55% | 5% | ~2.5% |
| simple | 50% | 59% | 9% | ~4.5% |
| **forced-eval** | 100% | 100% | 0% | **0%** |
| llm-eval | 100% | 100% | 0% | 0% |
| type-prompt | 41% | 55% | 14% | **~7%** |

### 통계적 유의성

구성당 약 50회 호출 기준:

| 비교 | 통계적으로 유의한가? | 신뢰도 |
|------|-------------------|--------|
| none vs forced-eval | ✅ 예 | >99.9% |
| none vs simple | ❌ 아니오 | ~60% |
| none vs type-prompt | ❌ 아니오 | ~70% |
| forced-eval vs llm-eval (표준) | ❌ 차이 없음 | 동일 (100%) |
| forced-eval vs llm-eval (엣지 오탐) | ✅ 예 | >99% |

### 3회 실행이 알려주는 것

구성당 3회 실행은 다음에 충분합니다:
- 100%와 ~50%를 구분 (명확한 신호)
- type-prompt가 유용하지 않음을 확인 (높은 분산, 개선 없음)
- 오탐 패턴 식별 (실행 간 일관성)

하지만 다음에는 충분하지 않습니다:
- 기준선 정밀 측정 (50% ± 5%는 넓음)
- simple vs none 비교 (노이즈 범위 내)
- forced-eval의 정확한 엣지 케이스 비율 결정

### 향후 연구 권장사항

더 좁은 신뢰 구간을 위해:
- 정밀한 기준선 측정을 위해 구성당 10회 이상 실행
- 오탐율을 위해 20개 이상의 엣지 케이스 프롬프트
- 일반화 가능성을 위한 교차 모델 테스트 (Sonnet 4, Opus)

---

## 4.6 지연 시간 분석

응답 시간은 개발자 경험에 영향을 미칩니다.

| 구성 | 평균 지연 | none 대비 오버헤드 | 설명 |
|------|----------|------------------|------|
| none | 8.7s | — | 기준선 |
| simple | 8.6s | -0.1s | 무시할 수준 |
| forced-eval | 10.7s | +2.0s | 평가 단계가 시간 추가 |
| llm-eval | 6.4s | -2.3s | Haiku 사전 필터링; Claude 평가 건너뜀 |
| type-prompt | 9.6s | +0.9s | 프롬프트 처리 오버헤드 |

### 2초의 트레이드오프

Forced-eval은 약 2초를 추가합니다:
- 각 Skill에 대해 YES/NO 작성
- 매칭된 Skill에 대해 Skill() 호출
- 구현 전 Skill 내용 읽기

2초가 100% 신뢰성의 가치가 있을까요? 대부분의 사용 사례에서 그렇습니다. Claude가 Skill 지식 없이 구현하는 비용(잘못된 패턴, 누락된 규칙)이 2초 대기보다 훨씬 큽니다.

### LLM-Eval의 속도 이점

LLM-eval이 가장 빠른 이유는 Haiku가 약 1초 만에 사전 분류하고 Claude가 자체 평가를 건너뛰기 때문입니다. 하지만 80% 오탐율은 그 속도가 엣지 케이스에서 정확도 비용으로 온다는 것을 의미합니다.

---

## 핵심 요약

- Hook 효과는 이진적: 100% (forced-eval, llm-eval) 또는 ~50% (나머지 전부)
- **Forced-eval은 모든 테스트 범주에서 오탐 0%** — 커밋먼트 메커니즘이 양방향 자제를 제공
- LLM-eval의 비매칭 프롬프트에서 80% 오탐율은 범용 사용에 위험
- 키워드 겹침이 기준선 활성화를 주도 — 의미 이해만으로는 불충분
- Forced-eval의 2초 지연 오버헤드는 신뢰성 향상 대비 무시할 수준
- Type-prompt Hook은 가장 높은 분산을 보이며 실제로 활성화율을 저하시킬 수 있음
- 구성당 3회 실행은 명확한 신호에 충분하지만 정밀 측정에는 부족

## 연습문제

### 연습문제 4.1: 결과 해석

새로운 Hook 구성 "structured-remind"에 대한 다음 결과가 주어졌습니다:

| 실행 | 표준 | 엣지 (매칭) | 엣지 (비매칭) |
|------|------|-----------|-------------|
| 1 | 75% | 50% | 10% |
| 2 | 83% | 42% | 15% |
| 3 | 67% | 58% | 5% |

1. 각 범주의 평균 활성화율을 계산하세요
2. 기준선보다 나은가요? 얼마나?
3. Forced-eval 대신 사용할 가치가 있나요? 왜 또는 왜 아닌가요?
4. 분산이 메커니즘에 대해 무엇을 시사하나요?

### 연습문제 4.2: 오탐 비용 분석

프로젝트에 각각 약 500 토큰의 내용을 가진 5개 Skill이 있습니다. LLM-eval의 80% 오탐율 기준:

1. 비매칭 프롬프트당 평균 몇 개의 Skill이 오활성화되나요?
2. 이로 인해 Claude의 컨텍스트에 얼마나 많은 추가 토큰이 추가되나요?
3. 입력 토큰 $3/MTok 기준, 비매칭 프롬프트 1,000건당 추가 비용은?
4. 이 비용이 수용 가능한가요? Skill이 각각 2,000 토큰이라면?

### 연습문제 4.3: 더 나은 엣지 케이스 테스트 설계

현재 엣지 케이스 세트는 4-5개 프롬프트만 있습니다. 다음을 포함하는 더 포괄적인 엣지 케이스 모음을 설계하세요:
- 의미만 프롬프트 5개 (관련 개념, 키워드 없음)
- 비매칭 프롬프트 5개 (완전히 무관)
- 모호한 프롬프트 5개 (어느 쪽이든 가능)

각각에 대해 forced-eval과 llm-eval의 예상 동작을 예측하세요.

---

## 다음 단계

[Module 5: 구현 가이드](./05-implementation-guide.ko.md)로 계속하여 자신의 Claude Code 프로젝트에 forced-eval을 설정하세요.
